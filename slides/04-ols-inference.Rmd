---
title: "Statistical Analysis in Political Science II:\\newline Inference"
author: "JBrandon Duck-Mayr"
date: "`r quack::american_date_format(Sys.Date())`"
urlcolor: Blue
header-includes:
    - \newcommand{\setsep}{\setlength{\itemsep}{3pt}}
    - \newcommand{\setskip}{\setlength{\parskip}{3pt}}
    - \renewcommand{\tightlist}{\setsep\setskip}
    - \newcommand{\expectation}[1]{\ensuremath{\mathbb{E}\left[#1\right]}}
    - \newcommand{\pd}[2][]{\ensuremath{\frac{\partial{#1}}{\partial{#2}}}}
    - \DeclareMathOperator{\Var}{Var}
    - \newcommand{\variance}[1]{\ensuremath{\Var\left[#1\right]}}
    - \DeclareMathOperator{\standarddeviation}{sd}
    - \DeclareMathOperator{\standarderror}{se}
output:
    quack::presentation:
        toc: false
        incremental: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
```

## Recap: Assumptions

::: nonincremental

1. Linear in parameters
2. Random sampling
3. No perfect collinearity: In the sample, no explanatory variable is constant, and there is no **exact** linear relationship between any explanatory variables
4. Zero conditional mean: The error term has the same expected value at any value of the explanatory variables: $\expectation{\varepsilon \mid x_1, \dots, x_k} = 0$
5. Homoscedasticity: The error term has the same variance at any value of the explanatory variables; $\variance{\varepsilon \mid x_1, \dots, x_k} = \sigma^2$

:::

## Assumption of Normality

. . .

- Now we will add a sixth assumption:

6.  $\varepsilon \sim \mathcal{N}\left( 0, \sigma^2 \right)$

- We can also write the assumption as:

\onslide<4->{
$$
y \mid \mathbf{x} \sim \mathcal{N}\left( \mathbf{x}\boldsymbol\beta, \sigma^2 \right)
$$
}

- This is a **much** stronger assumption than anything we've assumed yet
- *but*, we need the assumption for actual statistical inference

## ...wait, why isn't mean and variance enough??

. . .

For example, these two very different distributions have the same mean (1/2) and variance (1/3)

```{r diff-dist-w-same-moments, fig.align = 'center', out.width = "0.8\\textwidth"}
x = seq(from = 0, to = 2, by = 0.01)
y = dnorm(x, mean = 1/2, sd = sqrt(1/3))
z = dunif(x, min = 0, max = 2)
n = length(x)
g = c(N = "Normal(1/2, 1/3)", U = "Uniform(0, 2)")
v = c("#0072b2", "#d55e00")
w = c("solid",   "dashed")
d = data.frame(x = rep(x, 2), y = c(y, z), g = rep(g, each = n))
p = ggplot(data = d, mapping = aes(x = x, y = y, color = g, linetype = g)) +
    geom_line(linewidth = 1) +
    scale_color_manual(values = v, name = "Distribution") +
    scale_linetype_manual(values = w, name = "Distribution") +
    xlab("x") + ylab("Density") +
    theme_bw() +
    theme(
        axis.title  = element_text(size = 18),
        axis.text.x = element_text(size = 16),
        axis.text.y = element_blank(),
        legend.position  = "top",
        legend.direction = "horizontal",
        legend.text  = element_text(size = 20),
        legend.title = element_text(size = 20),
        legend.key.width = unit(3,"cm")
    )
p
```

## Central Limit Theorem

. . .

::: {.block}
### The Central Limit Theorem

Let $\left\{ y_1, y_2, \dots, y_n \right\}$ be a random sample with mean $\mu$ and variance $\sigma^2$. Then,

$$
Z_n = \frac{\bar{y}_n - \mu}{\sigma/\sqrt{n}} = \frac{\sqrt{n}\left(\bar{y}_n - \mu\right)}{\sigma}
$$

has an asymptotic standard normal distribution.
:::

## Distribution of $\hat{\boldsymbol\beta}$ when assuming Normality

. . .

::: {.block}
### Normal sampling distribution
Under assumptions 1--6, conditional on $\mathbf{X}$,
\begin{align*}
\hat{\beta}_j & \sim \mathcal{N}\left(\beta_j, \variance{\hat{\beta}_j}\right), \text{ so } \\
\frac{\hat{\beta}_j - \beta_j}{\standarddeviation{\left(\hat{\beta}_j\right)}} & \sim \mathcal{N}\left(0, 1\right).
\end{align*}
:::

. . .

::: {.block}
### $t$ distribution for the standardized estimators
Under assumptions 1--6, $$ \frac{\hat{\beta}_j - \beta_j}{\standarderror{\left(\hat{\beta}_j\right)}} \sim t_{n - k - 1}, $$
where $k + 1$ is the number of unknown parameters.
:::

## Testing hypotheses

. . .

- The predominant paradigm is null hypothesis significance testing (NHST)
- There are issues with NHST most political scientists ignore despite publications explaining them
- But you have to know how it works (and will likely use it despite my philosophical opposition, which is okay)

## Testing hypotheses about one parameter

. . .

- We usually test a **null hypothesis**, $\text{H}_0 : \beta_j = 0$
- Then we test that hypothesis using the test statistic
  $$ t_{\hat{\beta}_j} \equiv \hat{\beta}_j / \standarderror\left(\hat{\beta}_j\right) $$
- We reject $\text{H}_0$ when $t_{\hat{\beta}_j}$ is extreme enough
- That is, we set a *significance level*, like 0.05, that represents the probability of rejecting $\text{H}_0$ when it is true
- Then, given the distribution of $t_{\hat{\beta}_j}$, if there is a $t$-statistic extreme enough, we reject the null hypothesis

## What is a $p$-value?

. . .

- A $p$-value is the smallest significance level at which the null hypothesis would be rejected
- the p-value is the probability of observing a $t$ statistic as extreme as we did if the null hypothesis is true
- the p-value is the probability of observing a $t$ statistic as extreme as we did if the null hypothesis is true
- (I did that on purpose)
- $p$ values are uniformly distributed under the null hypothesis

## Confidence intervals

. . .

- A confidence interval is not a probability statement about your estimate
- The true unknown parameter value will be contained in 95% of 95% confidence intervals\only<4-5>{\dots}
- ...but you don't know whether it's in your confidence interval or not

## Linear combinations of parameters

. . .

- We may want to know something more complicated than whether $\beta_j \neq 0$
- For example, sometimes we want to know if $\beta_j = \beta_k$, or $\beta_j < \beta_k$, and other similar questions
- Consider $\beta_j < \beta_k$
  + Equivalent to $\beta_j - \beta_k < 0$
  + $t = \displaystyle\frac{\hat{\beta_j} - \hat{\beta_k}}{\text{se}\left(\hat{\beta_j} - \hat{\beta_k}\right)}$
  + But what is $\text{se}\left(\hat{\beta_j} - \hat{\beta_k}\right)$?
    * Recall $\text{Var}\left(X\right) + \text{Var}\left(Y\right) = \text{Var}\left(X\right) + \text{Var}\left(Y\right) + 2 \text{Cov}\left(X, Y\right)$

<!-- ## Reporting results -->
