---
title: "Statistical Analysis in Political Science II:\\newline Qualitative variables and measurement error"
author: "JBrandon Duck-Mayr"
date: "`r quack::american_date_format(Sys.Date())`"
urlcolor: Blue
header-includes:
    - \newcommand{\setsep}{\setlength{\itemsep}{3pt}}
    - \newcommand{\setskip}{\setlength{\parskip}{3pt}}
    - \renewcommand{\tightlist}{\setsep\setskip}
    - \newcommand{\expectation}[1]{\ensuremath{\mathbb{E}\left[#1\right]}}
    - \newcommand{\pd}[2][]{\ensuremath{\frac{\partial{#1}}{\partial{#2}}}}
    - \DeclareMathOperator{\Var}{Var}
    - \DeclareMathOperator{\Cov}{Cov}
    - \newcommand{\variance}[1]{\ensuremath{\Var\left[#1\right]}}
    - \DeclareMathOperator{\standarddeviation}{sd}
    - \DeclareMathOperator{\standarderror}{se}
    - \usepackage{siunitx}
output:
    quack::presentation:
        toc: false
        incremental: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Types of variables

. . .

- We have mostly focused on continuous predictors
- However, there are several types of data
  * Continuous, such as wages
  * Binary, such as whether a candidate is an incumbent
  * Categorical, such as party identification
  * Ordinal, such as education
  * And more
- Each has their own considerations

## Binary variables

. . .

- Also called dummy variables or dichotomous variables
- Encode a specific type of qualitative variable, one characteristic that can take only one of two values
- Examples (some better than others): treatment indicator, whether it's an election year, gender

\onslide<5>{
With dummy variable $D$ and other predictors $x_2, \dots, x_k$,
in the model
$$
y = \beta_0 + \beta_1 D + \beta_2 x_2 + \dots + \beta_k x_k + \varepsilon,
$$
the coefficient $\beta_1$ on $D$ represents an \textbf{intercept shift} when $D = 1$
}

## Categorical variables

. . .

- What about characteristics with more than one value, like party identification?
- Omit one category and create dummy variables for each other category
- Then we interpret the dummy coefficients as an intercept difference between that group and the omitted group
- Does it matter if the information is ordinal, like education?

## Ordinal variables

. . .

- We have a few choices with ordinal variables
- First consider simply adding $\beta_j x_j$ to a model where $x_j$ is ordinal
- What are the effect sizes for each level of the variable under dummy coding?
- Now consider ordinal coding

\onslide<6->{
Example with three levels:
$
\begin{pmatrix}
1 \\
2 \\
3
\end{pmatrix}
\rightarrow
\begin{pmatrix}
0 & 0 \\
1 & 0 \\
1 & 1
\end{pmatrix}
$
}

\onslide<7->{
Model: $y = \beta_0 + \beta_1 d_1 + \beta_2 d_2 + \varepsilon$, where $d_1 = 1$ if $x_i > 1$ and $d_2 = 1$ if $x_1 > 2$
}

\onslide<8>{
Then
$$
\hat{y}_i =
\begin{cases}
\beta_0 & \text{if } x_i = 1 \\
\beta_0 + \beta_1 & \text{if } x_i = 2 \\
\beta_0 + \beta_1 + \beta_2 & \text{if } x_i = 3 \\
\end{cases}
$$
}

## Dummy:continuous interactions

. . .

- (note we will cover this in more detail next time)
- In the model $$ y = \beta_0 + \beta_1 D + \beta_2 x + \beta_3 D x + \varepsilon, $$ $\beta_1$ represents an intercept shift when $D = 1$, but we also allow for **different slopes**
- Then $\beta_2$ is the slope on $x$ when $D = 0$ and $\beta_2 + \beta_3$ is the slope on $x$ when $D = 1$

## Testing differences across groups

. . .

- Suppose we want to let all the slopes differ across groups,
$$ y = \beta_0 + \beta_{g,1} x_1 + \beta_{g, 2} x_2 + \dots + \beta_{g, k} x_k + \varepsilon $$
- We can do an $F$-test with $k + 1$ restrictions
- We can also run separate regressions
  + Let $\text{SSR}_1$ be the SSR for a regression for the first group
  + Let $\text{SSR}_2$ be the SSR for a regression for the second
  + Let $\text{SSR}_p$ be the SSR for a pooled regression
  + Then we compute the $F$-statistic $$ F = \frac{\text{SSR}_p - (\text{SSR}_1 + \text{SSR}_2)}{\text{SSR}_1 + \text{SSR}_2} \frac{n - 2(k+1)}{k + 1}, $$ with $k$ and $n_1 + n_2 - 2k$ d.o.f.
  + This is called a Chow test

## Measurement error in the outcome

. . .

- Say we want to estimate the equation $y^\ast = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + \varepsilon,$ but only observe $y = y^\ast + e_0$
- Then we estimate $y = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + \varepsilon^\ast$, where $\varepsilon^\ast = \varepsilon + e_0$
- This is valid when $e_0$ is unrelated to the predictors
  + Notice the effect on estimator variance: $\variance{\varepsilon^\ast} = \variance{\varepsilon + e_0} = \sigma_\varepsilon^2 + \sigma_0^2 > \sigma_\varepsilon^2$
- This is **not** valid if $e_0$ is related to the predictors\onslide<7->{. Why?} \onslide<8>{Recall OLS Assumption 4: Zero conditional mean (The error term has the same expected value at any value of the explanatory variables)}

## Measurement error in the predictors

. . .

- Say we want to estimate the equation $y = \beta_0 + \beta_1 x_1^\ast + \dots + \beta_k x_k + \varepsilon,$ but only observe $x_1 = x_1^\ast + e_1$
- Two cases: 
- $\Cov\left(x_1, e_1\right) = 0$
  + Estimate $y = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + \left(\varepsilon - \beta_1 e_1\right)$
  + Still unbiased
  + Variance increases: $\variance{\varepsilon - \beta_1 e_1} = \sigma_\varepsilon^2 + \beta_1 \sigma_{e_1}^2$
- $\Cov\left(x_1^\ast, e_1\right) = 0$
  + classical errors-in-variables (CEV) assumption
  + then $e_1$ and $x_1$ are correlated 
  + covariance between $x_1$ and $\left(\varepsilon - \beta_1 e_1\right)$ is
    $$ \Cov\left(x_1, \varepsilon - \beta_1 e_1\right) = -\beta_1 \Cov\left(x_1, e_1\right) = \beta_1\sigma_{e_1}^2$$

## Wait! A bit of background

. . .

- I am about to use notation $\text{plim}$. What does that mean?
- Take a random variable sequence $\{X_1, X_2, \dots, X_n\}$
- The sequence *weakly converges* to a constant $c$ if
  $$ \lim_{n\to\infty} \Pr\left(\mid X_n - c \mid > \varepsilon\right) = 0 $$ for any $\varepsilon > 0$
- We also write $\text{plim}_{n\to\infty} X_n = c$

## Measurement error in the predictors

. . .

Then in the two variable case,

\begin{align*}
\text{plim}\left(\hat{\beta}_1\right)
& = \beta_1 + \frac{\Cov\left(x_1, \varepsilon - \beta_1 e_1\right)}{\variance{x_1}} \\
& = \beta_1 - \frac{\beta_1\sigma_{e_1}^2}{\sigma_{x_1^\ast}^2 + \sigma_{e_1}^2} \\
& = \beta_1 \left( \frac{\sigma_{x_1^\ast}^2}{\sigma_{x_1^\ast}^2 + \sigma_{e_1}^2} \right)
\end{align*}

. . .

For $k$ variables,

$$
\text{plim}\left(\hat{\beta}_1\right)
=
\beta_1 \left( \frac{\sigma_{r_1^\ast}^2}{\sigma_{r_1^\ast}^2 + \sigma_{e_1}^2} \right)
$$

## Missing data

. . .

- Different types of missingness:
  + \textcolor<7-8>{BurntOrange}{MCAR (missing completely at random): missingness unrelated to observed \textit{and} unobserved data}
  + \textcolor<8>{BurntOrange}{MAR (missing at random): missingness only unrelated to unobserved data}
  + MNAR (missing not at random): missingess systematically related even to unobserved data
- Strategies to deal with missingness:
  + listwise deletion
  + multiple imputation
- What about MNAR data? Must jointly model response and missingness
- Missingness in predictors vs outcomes?
